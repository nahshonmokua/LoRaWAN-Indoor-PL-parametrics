{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4570cdd0-8797-4f9f-a519-68e84b2c2d0c",
   "metadata": {},
   "source": [
    "<p style=\"font-family: 'Courier New', Courier, monospace; font-size: 30px; font-weight: bold; color: blue;  text-align: left;\">\n",
    " Parametric Fitting + Regularization Methods for Indoor LoRaWAN Signal Propagation \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be56886-b2c1-4817-96a1-30d0c5950bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Core & Data Libraries ==============================\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================== Machine Learning & Stats ===========================\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# neat prints\n",
    "np.set_printoptions(suppress=True, linewidth=120)\n",
    "\n",
    "# fixed seed for CV components that use randomness\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531c7f2-e3bb-4690-a7c6-262c822cd4e9",
   "metadata": {},
   "source": [
    "###  Load data + fold index + basic split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d009bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60483, Test samples: 15121\n",
      "Fold distribution: {np.int64(0): np.int64(241929), np.int64(1): np.int64(241929), np.int64(2): np.int64(241929), np.int64(3): np.int64(241928), np.int64(4): np.int64(241928)}\n"
     ]
    }
   ],
   "source": [
    "# Path to the standardized database directory\n",
    "base_path = '../Extended Parametric Regression Files+Plots.'\n",
    "\n",
    "# Load train and test splits# Path to standardized data\n",
    "base_path = '../Extended Parametric Regression Files+Plots.'\n",
    "\n",
    "# Train/test splits\n",
    "df_train = pd.read_csv(f\"{base_path}/train.csv\")\n",
    "df_test  = pd.read_csv(f\"{base_path}/test.csv\")\n",
    "\n",
    "df_train = df_train.sort_values('time').reset_index(drop=True).iloc[::20].reset_index(drop=True)\n",
    "df_test  = df_test.sort_values('time').reset_index(drop=True).iloc[::20].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Raw feature set (distance, freq, walls, env, snr)\n",
    "feature_names = [\n",
    "    'distance', 'frequency', 'c_walls', 'w_walls', 'co2', 'humidity',\n",
    "    'pm25', 'pressure', 'temperature', 'snr'\n",
    "]\n",
    "\n",
    "X_train_raw = df_train[feature_names].values\n",
    "y_train     = df_train['PL'].values\n",
    "X_test_raw  = df_test[feature_names].values\n",
    "y_test      = df_test['PL'].values\n",
    "\n",
    "# Optional time columns if needed later\n",
    "time_train = df_train.get('time', pd.Series(np.arange(len(df_train)))).values\n",
    "time_test  = df_test.get('time',  pd.Series(np.arange(len(df_test)))).values\n",
    "\n",
    "print(f\"Training samples: {X_train_raw.shape[0]}, Test samples: {X_test_raw.shape[0]}\")\n",
    "\n",
    "# Leakage-safe fold assignments (grouped-by-device, time-blocked) prepared offline\n",
    "fold_assignments = np.load(f\"{base_path}/train_folds.npy\")\n",
    "unique, counts = np.unique(fold_assignments, return_counts=True)\n",
    "print(\"Fold distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd2a2b-88de-4bd9-b96f-b17522bfd007",
   "metadata": {},
   "source": [
    "### Model linearization (log-distance + freq offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66936ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearized feature dimensionality (p): 9\n"
     ]
    }
   ],
   "source": [
    "# Linearization separates the non-linear frequency term and transforms distance\n",
    "# LDPL: pull 20*log10(f) to the RHS as a fixed offset; transform distance to 10*log10(d/d0)\n",
    "d0 = 1.0\n",
    "\n",
    "# Train transforms\n",
    "log_d_train  = np.log10(np.clip(X_train_raw[:, 0], 1e-9, None) / d0)\n",
    "offset_train = 20.0 * np.log10(np.clip(X_train_raw[:, 1], 1e-9, None))\n",
    "X_lin_train  = np.column_stack([\n",
    "    10.0 * log_d_train,           # index 0: distance term for exponent n\n",
    "    X_train_raw[:, 2:10]          # indices 1..8: c_walls, w_walls, co2, humidity, pm25, pressure, temperature, snr\n",
    "])\n",
    "y_train_adj  = y_train - offset_train\n",
    "\n",
    "# Test transforms\n",
    "log_d_test  = np.log10(np.clip(X_test_raw[:, 0], 1e-9, None) / d0)\n",
    "offset_test = 20.0 * np.log10(np.clip(X_test_raw[:, 1], 1e-9, None))\n",
    "X_lin_test  = np.column_stack([\n",
    "    10.0 * log_d_test,\n",
    "    X_test_raw[:, 2:10]\n",
    "])\n",
    "y_test_adj  = y_test - offset_test\n",
    "\n",
    "p = X_lin_train.shape[1]\n",
    "print(f\"Linearized feature dimensionality (p): {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e28a6-d1df-40cc-8f6e-6344c7a0feb3",
   "metadata": {},
   "source": [
    "### Model builders (linear + poly(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfbdb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Linear (degree=1): OLS + L2 + L1 + EN ----------\n",
    "def build_mlr_linear():\n",
    "    # Standardize features; plain OLS on adjusted target\n",
    "    return make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        LinearRegression()\n",
    "    )\n",
    "\n",
    "def build_ridge_linear():\n",
    "    alphas = np.logspace(-6, 6, 100)\n",
    "    return make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')\n",
    "    )\n",
    "\n",
    "def build_lasso_linear():\n",
    "    alphas = np.logspace(-6, 2, 60)\n",
    "    return make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        LassoCV(alphas=alphas, cv=5, random_state=RANDOM_STATE, max_iter=200000)\n",
    "    )\n",
    "\n",
    "def build_enet_linear():\n",
    "    alphas = np.logspace(-6, 2, 40)\n",
    "    l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    return make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5,\n",
    "                     random_state=RANDOM_STATE, max_iter=300000, tol=1e-3,\n",
    "                     n_jobs=-1, precompute='auto')\n",
    "    )\n",
    "\n",
    "# ---- degree-2 expansion only on continuous vars + SNR; walls kept linear ----------\n",
    "# X_lin columns: [0]=distance, [1]=c_walls, [2]=w_walls, [3]=co2, [4]=humidity, [5]=pm25, [6]=pressure, [7]=temperature, [8]=snr\n",
    "POLY_KEEP_LINEAR_IDXS = [1, 2]                      # keep walls linear\n",
    "POLY_EXPAND_IDXS      = [0, 3, 4, 5, 6, 7, 8]       # expand distance + env + SNR \n",
    "\n",
    "class PolyOnContinuous(TransformerMixin):\n",
    "    def __init__(self, keep_idxs=POLY_KEEP_LINEAR_IDXS, expand_idxs=POLY_EXPAND_IDXS, degree=2):\n",
    "        self.keep_idxs = keep_idxs\n",
    "        self.expand_idxs = expand_idxs\n",
    "        self.degree = degree\n",
    "        self.poly_ = None\n",
    "    def fit(self, X, y=None):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        self.poly_ = PolynomialFeatures(self.degree, include_bias=False).fit(X[:, self.expand_idxs])\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        X_lin = X[:, self.keep_idxs]\n",
    "        X_cont_poly = self.poly_.transform(X[:, self.expand_idxs])\n",
    "        return np.hstack([X_lin, X_cont_poly])\n",
    "\n",
    "def build_poly_regr():\n",
    "    # OLS on degree-2 map (squares + pairwise interactions); walls remain additive in X\n",
    "    return make_pipeline(\n",
    "        PolyOnContinuous(),\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        LinearRegression()\n",
    "    )\n",
    "\n",
    "def build_ridge_poly2():\n",
    "    alphas = np.logspace(-6, 6, 100)\n",
    "    return make_pipeline(\n",
    "        PolyOnContinuous(),\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        RidgeCV(alphas=alphas, cv=5, scoring='neg_mean_squared_error')\n",
    "    )\n",
    "\n",
    "def build_lasso_poly2():\n",
    "    alphas = np.logspace(-6, 0, 50)\n",
    "    return make_pipeline(\n",
    "        PolyOnContinuous(),\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        LassoCV(alphas=alphas, cv=5, random_state=RANDOM_STATE, max_iter=200000)\n",
    "    )\n",
    "\n",
    "def build_enet_poly2():\n",
    "    alphas = np.logspace(-3, 0, 30)\n",
    "    l1_ratios = [0.2, 0.4, 0.6, 0.8]\n",
    "    return make_pipeline(\n",
    "        PolyOnContinuous(),\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        ElasticNetCV(alphas=alphas, l1_ratio=l1_ratios, cv=5,\n",
    "                     random_state=RANDOM_STATE, max_iter=300000, tol=1e-3,\n",
    "                     n_jobs=-1, precompute='auto')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1cc27-a778-4fb4-9499-d5b336a01c2b",
   "metadata": {},
   "source": [
    "### Full conjugate BLR on the linear design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ad3c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullBLRConjugate(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Conjugate Bayesian Linear Regression with Normal-Inverse-Gamma prior:\n",
    "        beta | sigma^2 ~ N(beta0, sigma^2 V0)\n",
    "        sigma^2 ~ Inv-Gamma(a0, b0)\n",
    "    Works on adjusted target; intercept added internally.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta0=None, V0_scale=1e6, a0=1e-2, b0=1e-2):\n",
    "        self.beta0 = beta0\n",
    "        self.V0_scale = V0_scale\n",
    "        self.a0 = a0\n",
    "        self.b0 = b0\n",
    "        self.beta_n_ = None\n",
    "        self.Vn_ = None\n",
    "        self.an_ = None\n",
    "        self.bn_ = None\n",
    "\n",
    "    def _augment(self, X):\n",
    "        n = X.shape[0]\n",
    "        return np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float).reshape(-1)\n",
    "\n",
    "        X_aug = self._augment(X)   # [n x (p+1)]\n",
    "        n, d = X_aug.shape\n",
    "\n",
    "        if self.beta0 is None:\n",
    "            self.beta0 = np.zeros(d)\n",
    "        beta0 = self.beta0\n",
    "        \n",
    "        # prior covariance V0 = V0_scale * I  (weakly informative on standardized X)\n",
    "        V0_inv = np.eye(d) / self.V0_scale\n",
    "        XtX = X_aug.T @ X_aug\n",
    "        Vn_inv = V0_inv + XtX\n",
    "        Vn = inv(Vn_inv)\n",
    "\n",
    "        Xty = X_aug.T @ y\n",
    "        beta_n = Vn @ (V0_inv @ beta0 + Xty)\n",
    "\n",
    "        an = self.a0 + 0.5 * n\n",
    "        resid = y - X_aug @ beta_n\n",
    "        term = (beta_n - beta0).T @ V0_inv @ (beta_n - beta0)\n",
    "        bn = self.b0 + 0.5 * (resid @ resid + term)\n",
    "\n",
    "        self.beta_n_ = beta_n\n",
    "        self.Vn_ = Vn\n",
    "        self.an_ = an\n",
    "        self.bn_ = bn\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, return_std=False):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        X_aug = self._augment(X)\n",
    "        mean = X_aug @ self.beta_n_\n",
    "        if not return_std:\n",
    "            return mean\n",
    "            \n",
    "        # predictive var = (bn/an) * (1 + x^T Vn x)    \n",
    "        pred_var = (self.bn_ / self.an_) * (1.0 + np.sum(X_aug @ self.Vn_ * X_aug, axis=1))\n",
    "        pred_std = np.sqrt(np.maximum(pred_var, 0.0))\n",
    "        return mean, pred_std\n",
    "\n",
    "def build_full_blr_linear():\n",
    "    # Standardize first; BLR on adjusted target. This is *not* sklearn's BayesianRidge.\n",
    "    return make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True),\n",
    "        FullBLRConjugate(beta0=None, V0_scale=1e6, a0=1e-2, b0=1e-2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835fe41-904c-4ac0-8c62-7767d7c2964f",
   "metadata": {},
   "source": [
    "### Model dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "122eb6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined: ['MLR_linear', 'Ridge_linear', 'Lasso_linear', 'ElasticNet_linear', 'Poly2_OLS', 'Ridge_poly2', 'Lasso_poly2', 'ElasticNet_poly2', 'BLR_linear']\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    # Linear (degree=1)\n",
    "    \"MLR_linear\":         build_mlr_linear(),\n",
    "    \"Ridge_linear\":       build_ridge_linear(),\n",
    "    \"Lasso_linear\":       build_lasso_linear(),\n",
    "    \"ElasticNet_linear\":  build_enet_linear(),\n",
    "\n",
    "    # Polynomial (degree=2) on distance + env + SNR; walls linear\n",
    "    \"Poly2_OLS\":          build_poly_regr(),\n",
    "    \"Ridge_poly2\":        build_ridge_poly2(),\n",
    "    \"Lasso_poly2\":        build_lasso_poly2(),\n",
    "    \"ElasticNet_poly2\":   build_enet_poly2(),\n",
    "\n",
    "    # Full conjugate BLR on linear design\n",
    "    \"BLR_linear\":         build_full_blr_linear(),\n",
    "}\n",
    "print(\"Models defined:\", list(models.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a80f9c-e03f-4487-b524-19f9d99f8b4a",
   "metadata": {},
   "source": [
    "### Leakage‑safe 5‑fold CV (outer) and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cf29459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MLR_linear | RMSE_val: 8.205 ± 0.068 | R2_val: 0.816 ± 0.003\n",
      "    Ridge_linear | RMSE_val: 8.210 ± 0.066 | R2_val: 0.816 ± 0.003\n",
      "    Lasso_linear | RMSE_val: 8.205 ± 0.068 | R2_val: 0.816 ± 0.003\n",
      "ElasticNet_linear | RMSE_val: 8.215 ± 0.067 | R2_val: 0.816 ± 0.003\n",
      "       Poly2_OLS | RMSE_val: 7.164 ± 0.090 | R2_val: 0.860 ± 0.003\n",
      "     Ridge_poly2 | RMSE_val: 7.181 ± 0.093 | R2_val: 0.859 ± 0.003\n",
      "     Lasso_poly2 | RMSE_val: 7.174 ± 0.092 | R2_val: 0.859 ± 0.003\n",
      "ElasticNet_poly2 | RMSE_val: 7.183 ± 0.093 | R2_val: 0.859 ± 0.003\n",
      "      BLR_linear | RMSE_val: 8.205 ± 0.068 | R2_val: 0.816 ± 0.003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>RMSE_train_mean</th>\n",
       "      <th>RMSE_train_std</th>\n",
       "      <th>RMSE_val_mean</th>\n",
       "      <th>RMSE_val_std</th>\n",
       "      <th>R2_train_mean</th>\n",
       "      <th>R2_train_std</th>\n",
       "      <th>R2_val_mean</th>\n",
       "      <th>R2_val_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Poly2_OLS</td>\n",
       "      <td>7.152488</td>\n",
       "      <td>0.022314</td>\n",
       "      <td>7.163984</td>\n",
       "      <td>0.089899</td>\n",
       "      <td>0.860196</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.859738</td>\n",
       "      <td>0.002936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lasso_poly2</td>\n",
       "      <td>7.163792</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>7.173627</td>\n",
       "      <td>0.092082</td>\n",
       "      <td>0.859754</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.859358</td>\n",
       "      <td>0.003079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge_poly2</td>\n",
       "      <td>7.171767</td>\n",
       "      <td>0.020789</td>\n",
       "      <td>7.181409</td>\n",
       "      <td>0.093172</td>\n",
       "      <td>0.859442</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.859052</td>\n",
       "      <td>0.003133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ElasticNet_poly2</td>\n",
       "      <td>7.174211</td>\n",
       "      <td>0.020688</td>\n",
       "      <td>7.183040</td>\n",
       "      <td>0.092996</td>\n",
       "      <td>0.859346</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.858988</td>\n",
       "      <td>0.003134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BLR_linear</td>\n",
       "      <td>8.203852</td>\n",
       "      <td>0.016890</td>\n",
       "      <td>8.204890</td>\n",
       "      <td>0.068137</td>\n",
       "      <td>0.816075</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.816018</td>\n",
       "      <td>0.002954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLR_linear</td>\n",
       "      <td>8.203852</td>\n",
       "      <td>0.016890</td>\n",
       "      <td>8.204890</td>\n",
       "      <td>0.068137</td>\n",
       "      <td>0.816075</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.816018</td>\n",
       "      <td>0.002954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso_linear</td>\n",
       "      <td>8.204550</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>8.205498</td>\n",
       "      <td>0.068066</td>\n",
       "      <td>0.816044</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.815990</td>\n",
       "      <td>0.002959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge_linear</td>\n",
       "      <td>8.209306</td>\n",
       "      <td>0.017996</td>\n",
       "      <td>8.210337</td>\n",
       "      <td>0.066245</td>\n",
       "      <td>0.815831</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.815774</td>\n",
       "      <td>0.002895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElasticNet_linear</td>\n",
       "      <td>8.214237</td>\n",
       "      <td>0.016834</td>\n",
       "      <td>8.215207</td>\n",
       "      <td>0.067179</td>\n",
       "      <td>0.815610</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.815555</td>\n",
       "      <td>0.002950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  RMSE_train_mean  RMSE_train_std  RMSE_val_mean  \\\n",
       "4          Poly2_OLS         7.152488        0.022314       7.163984   \n",
       "6        Lasso_poly2         7.163792        0.021398       7.173627   \n",
       "5        Ridge_poly2         7.171767        0.020789       7.181409   \n",
       "7   ElasticNet_poly2         7.174211        0.020688       7.183040   \n",
       "8         BLR_linear         8.203852        0.016890       8.204890   \n",
       "0         MLR_linear         8.203852        0.016890       8.204890   \n",
       "2       Lasso_linear         8.204550        0.016997       8.205498   \n",
       "1       Ridge_linear         8.209306        0.017996       8.210337   \n",
       "3  ElasticNet_linear         8.214237        0.016834       8.215207   \n",
       "\n",
       "   RMSE_val_std  R2_train_mean  R2_train_std  R2_val_mean  R2_val_std  \n",
       "4      0.089899       0.860196      0.000730     0.859738    0.002936  \n",
       "6      0.092082       0.859754      0.000706     0.859358    0.003079  \n",
       "5      0.093172       0.859442      0.000687     0.859052    0.003133  \n",
       "7      0.092996       0.859346      0.000688     0.858988    0.003134  \n",
       "8      0.068137       0.816075      0.000733     0.816018    0.002954  \n",
       "0      0.068137       0.816075      0.000733     0.816018    0.002954  \n",
       "2      0.068066       0.816044      0.000741     0.815990    0.002959  \n",
       "1      0.066245       0.815831      0.000786     0.815774    0.002895  \n",
       "3      0.067179       0.815610      0.000733     0.815555    0.002950  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure fold_assignments matches current training size (after any downsampling)\n",
    "fold_assignments = np.array(fold_assignments)[:X_lin_train.shape[0]]\n",
    "\n",
    "def reconstruct_full(y_pred_adj, offset):\n",
    "    # put back 20*log10(f) to compare in PL units (dB)\n",
    "    return y_pred_adj + offset\n",
    "\n",
    "cv_results = []\n",
    "for name, pipe in models.items():\n",
    "    rmse_tr, rmse_val = [], []\n",
    "    r2_tr, r2_val = [], []\n",
    "    for fold in range(5):\n",
    "        tr_idx = np.where(fold_assignments != fold)[0]\n",
    "        val_idx = np.where(fold_assignments == fold)[0]\n",
    "\n",
    "        X_tr, y_tr, y_tr_adj, off_tr = (\n",
    "            X_lin_train[tr_idx], y_train[tr_idx], y_train_adj[tr_idx], offset_train[tr_idx]\n",
    "        )\n",
    "        X_val, y_val, y_val_adj, off_val = (\n",
    "            X_lin_train[val_idx], y_train[val_idx], y_train_adj[val_idx], offset_train[val_idx]\n",
    "        )\n",
    "\n",
    "        est = clone(pipe).fit(X_tr, y_tr_adj)\n",
    "\n",
    "        # Train metrics in PL units\n",
    "        y_tr_pred = reconstruct_full(est.predict(X_tr), off_tr)\n",
    "        rmse_tr.append(np.sqrt(mean_squared_error(y_tr, y_tr_pred)))\n",
    "        r2_tr.append(r2_score(y_tr, y_tr_pred))\n",
    "\n",
    "        # Validation metrics in PL units\n",
    "        y_val_pred = reconstruct_full(est.predict(X_val), off_val)\n",
    "        rmse_val.append(np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
    "        r2_val.append(r2_score(y_val, y_val_pred))\n",
    "\n",
    "    row = {\n",
    "        \"model\": name,\n",
    "        \"RMSE_train_mean\": np.mean(rmse_tr), \"RMSE_train_std\": np.std(rmse_tr),\n",
    "        \"RMSE_val_mean\":   np.mean(rmse_val), \"RMSE_val_std\":   np.std(rmse_val),\n",
    "        \"R2_train_mean\":   np.mean(r2_tr),    \"R2_train_std\":   np.std(r2_tr),\n",
    "        \"R2_val_mean\":     np.mean(r2_val),   \"R2_val_std\":     np.std(r2_val),\n",
    "    }\n",
    "    cv_results.append(row)\n",
    "    print(f\"{name:>16} | RMSE_val: {row['RMSE_val_mean']:.3f} ± {row['RMSE_val_std']:.3f} | \"\n",
    "          f\"R2_val: {row['R2_val_mean']:.3f} ± {row['R2_val_std']:.3f}\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results).sort_values(by=\"RMSE_val_mean\")\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331e6e3-7361-416c-9d12-0b8f1af25fdf",
   "metadata": {},
   "source": [
    "### Fit full‑train models + persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f048a8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models: ['MLR_linear', 'Ridge_linear', 'Lasso_linear', 'ElasticNet_linear', 'Poly2_OLS', 'Ridge_poly2', 'Lasso_poly2', 'ElasticNet_poly2', 'BLR_linear']\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('Models', exist_ok=True)\n",
    "fitted_models = {}\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    est = clone(pipe).fit(X_lin_train, y_train_adj)\n",
    "    fitted_models[name] = est\n",
    "    with open(f\"Models/{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(est, f)\n",
    "\n",
    "print(\"Saved models:\", list(fitted_models.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a741e5a-3c82-4559-854d-c58d1f0e78c7",
   "metadata": {},
   "source": [
    "### Test‑set evaluation + summaries + poly term count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a08be0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            model  RMSE_test  R2_test\n",
      "        Poly2_OLS   7.453534 0.850274\n",
      "      Lasso_poly2   7.472610 0.849506\n",
      " ElasticNet_poly2   7.485781 0.848975\n",
      "      Ridge_poly2   7.486178 0.848959\n",
      "       MLR_linear   8.486832 0.805882\n",
      "       BLR_linear   8.486832 0.805882\n",
      "     Lasso_linear   8.489267 0.805771\n",
      "     Ridge_linear   8.492559 0.805620\n",
      "ElasticNet_linear   8.498172 0.805363\n",
      "Degree-2 expanded feature count (walls kept linear): 37\n",
      "\n",
      "Best by CV (lowest RMSE_val_mean): Poly2_OLS\n",
      "Best on Test (lowest RMSE_test):    Poly2_OLS\n"
     ]
    }
   ],
   "source": [
    "test_rows = []\n",
    "for name, est in fitted_models.items():\n",
    "    y_pred_adj = est.predict(X_lin_test)\n",
    "    y_pred = y_pred_adj + offset_test\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    test_rows.append({\"model\": name, \"RMSE_test\": rmse, \"R2_test\": r2})\n",
    "\n",
    "test_df = pd.DataFrame(test_rows).sort_values(by=\"RMSE_test\")\n",
    "print(test_df.to_string(index=False))\n",
    "\n",
    "# Persist summaries\n",
    "cv_df.to_csv(\"Models/cv_summary.csv\", index=False)\n",
    "test_df.to_csv(\"Models/test_summary.csv\", index=False)\n",
    "\n",
    "# Show final degree-2 feature count with partial expansion (walls linear)\n",
    "tmp_poly_pipe = PolyOnContinuous().fit(X_lin_train)\n",
    "n_poly_feats = tmp_poly_pipe.transform(X_lin_train[:1]).shape[1]\n",
    "print(\"Degree-2 expanded feature count (walls kept linear):\", n_poly_feats)\n",
    "\n",
    "# Choose best-by-CV and best-by-Test\n",
    "best_cv  = cv_df.iloc[0][\"model\"]\n",
    "best_tst = test_df.iloc[0][\"model\"]\n",
    "print(f\"\\nBest by CV (lowest RMSE_val_mean): {best_cv}\")\n",
    "print(f\"Best on Test (lowest RMSE_test):    {best_tst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "863d4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== FM Calibration Utilities ==========================\n",
    "import math\n",
    "from math import erf, sqrt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "try:\n",
    "    import scipy.stats as sps\n",
    "except Exception:\n",
    "    sps = None  # code falls back to empirical-only if SciPy unavailable\n",
    "\n",
    "# ---- Config ----\n",
    "P_GRID = [0.05, 0.02, 0.01]        # p in {5%, 2%, 1%}\n",
    "HEURISTIC_FM_DB = 10.0             # for baseline comparison (adjust as you like)\n",
    "RNG = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "# ---- Helpers ----\n",
    "def normal_cdf(z):\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    if sps is not None:\n",
    "        # SciPy available: exact and vectorized\n",
    "        return sps.norm.cdf(z)\n",
    "    # Fallback: accurate, fully vectorized GELU-style approximation\n",
    "    # Φ(z) ≈ 0.5 * (1 + tanh(√(2/π)*(z + 0.044715 z^3)))\n",
    "    return 0.5 * (1.0 + np.tanh(0.7978845608 * (z + 0.044715 * z**3)))\n",
    "\n",
    "def acf(x, max_lag=None):\n",
    "    x = np.asarray(x, float)\n",
    "    x = x - x.mean()\n",
    "    n = len(x)\n",
    "    if max_lag is None:\n",
    "        max_lag = min(200, n - 2) if n > 2 else 1\n",
    "    c = np.correlate(x, x, mode='full')[n-1:n+max_lag]\n",
    "    if c[0] == 0:\n",
    "        return np.arange(max_lag+1), np.ones(max_lag+1)\n",
    "    return np.arange(max_lag+1), c / c[0]\n",
    "\n",
    "def choose_block_len_acf(x, alpha=0.05, max_lag=None):\n",
    "    lags, ac = acf(x, max_lag=max_lag)\n",
    "    n = len(x)\n",
    "    if n < 50:\n",
    "        return 0, {\"note\": \"sample too small; using i.i.d.\"}\n",
    "    thr = 1.96 / math.sqrt(n)  # approximate 95% bounds\n",
    "    # first lag where |acf| drops below threshold\n",
    "    idx = np.where(np.abs(ac[1:]) < thr)[0]\n",
    "    if len(idx) == 0:\n",
    "        k = min(len(ac)-1, 25)\n",
    "    else:\n",
    "        k = int(idx[0] + 1)\n",
    "    b = max(5, k)\n",
    "    return b, {\"thr\": thr, \"first_below_thr_lag\": k}\n",
    "\n",
    "def mbb_resample(x, block_len, n=None, rng=None):\n",
    "    rng = RNG if rng is None else rng\n",
    "    x = np.asarray(x)\n",
    "    n = len(x) if n is None else n\n",
    "    b = int(block_len)\n",
    "    if b <= 1 or b >= n:\n",
    "        # degenerate: fall back to i.i.d.\n",
    "        idx = rng.integers(0, len(x), size=n)\n",
    "        return x[idx]\n",
    "    k = int(np.ceil(n / b))\n",
    "    starts = rng.integers(0, len(x) - b + 1, size=k)\n",
    "    out = np.concatenate([x[s:s+b] for s in starts])[:n]\n",
    "    return out\n",
    "\n",
    "def bca_ci(x, stat_fn, alpha=0.05, B=10_000, block_len=0, rng=None):\n",
    "    \"\"\"BCa CI for a statistic stat_fn over sample x. Uses MBB if block_len>0.\"\"\"\n",
    "    rng = RNG if rng is None else rng\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    theta_hat = stat_fn(x)\n",
    "\n",
    "    # bootstrap replicates\n",
    "    thetas = np.empty(B)\n",
    "    for b in range(B):\n",
    "        xb = mbb_resample(x, block_len, n=n, rng=rng) if block_len > 0 else x[rng.integers(0, n, size=n)]\n",
    "        thetas[b] = stat_fn(xb)\n",
    "    thetas.sort()\n",
    "\n",
    "    # bias-correction z0\n",
    "    if sps is not None:\n",
    "        z0 = sps.norm.ppf((thetas < theta_hat).mean() + 1e-12)\n",
    "    else:\n",
    "        z0 = 0.0  # fall back to percentile CI if SciPy not present\n",
    "\n",
    "    # jackknife for acceleration 'a'\n",
    "    jack = np.empty(n)\n",
    "    for i in range(n):\n",
    "        jack[i] = stat_fn(np.delete(x, i))\n",
    "    jack_mean = jack.mean()\n",
    "    num = np.sum((jack_mean - jack)**3)\n",
    "    den = 6.0 * (np.sum((jack_mean - jack)**2) ** 1.5)\n",
    "    a = (num / den) if den > 0 else 0.0\n",
    "\n",
    "    # adjusted alpha levels\n",
    "    al = alpha/2\n",
    "    au = 1 - alpha/2\n",
    "    if sps is not None:\n",
    "        zal = sps.norm.ppf(al); zau = sps.norm.ppf(au)\n",
    "        def _adj(z): return sps.norm.cdf(z0 + (z0 + z) / (1 - a*(z0 + z)))\n",
    "        a1 = float(_adj(zal)); a2 = float(_adj(zau))\n",
    "    else:\n",
    "        a1, a2 = al, au  # percentile CI\n",
    "\n",
    "    lo = thetas[int(np.floor((len(thetas)-1) * a1))]\n",
    "    hi = thetas[int(np.floor((len(thetas)-1) * a2))]\n",
    "    return float(lo), float(hi), {\"theta_hat\": float(theta_hat), \"z0\": float(z0), \"a\": float(a)}\n",
    "\n",
    "# ---- Tail models (Student-t, GMM) and quantiles ----\n",
    "def t_fit_quantile(sample, p):\n",
    "    if sps is None:\n",
    "        return np.nan, None, np.inf, np.inf\n",
    "    df, loc, scale = sps.t.fit(sample)\n",
    "    q = float(sps.t.ppf(1 - p, df, loc, scale))\n",
    "    logL = float(sps.t.logpdf(sample, df, loc, scale).sum())\n",
    "    k = 3; n = len(sample)\n",
    "    aic = 2*k - 2*logL\n",
    "    bic = k*np.log(n) - 2*logL\n",
    "    return q, (df, loc, scale), aic, bic\n",
    "\n",
    "def mixture_quantile(weights, means, stds, q, tol=1e-6, max_iter=256):\n",
    "    lo = float(means.min() - 10*stds.max())\n",
    "    hi = float(means.max() + 10*stds.max())\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5*(lo+hi)\n",
    "        F = float(np.sum(weights * normal_cdf((mid - means)/stds)))\n",
    "        if F < q: lo = mid\n",
    "        else:     hi = mid\n",
    "        if hi - lo < tol: break\n",
    "    return 0.5*(lo+hi)\n",
    "\n",
    "def gmm_fit_quantile(sample, p, max_k=3):\n",
    "    xs = sample.reshape(-1,1)\n",
    "    best = None\n",
    "    for k in range(1, max_k+1):\n",
    "        gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=RANDOM_STATE)\n",
    "        gmm.fit(xs)\n",
    "        bic = gmm.bic(xs); aic = gmm.aic(xs)\n",
    "        if (best is None) or (bic < best[\"bic\"]):\n",
    "            best = {\"gmm\": gmm, \"k\": k, \"bic\": float(bic), \"aic\": float(aic)}\n",
    "    w = best[\"gmm\"].weights_\n",
    "    mu = best[\"gmm\"].means_.ravel()\n",
    "    cov = best[\"gmm\"].covariances_\n",
    "    if cov.ndim == 3:\n",
    "        stds = np.sqrt(cov.reshape(-1))\n",
    "    else:\n",
    "        stds = np.sqrt(cov)\n",
    "    q = mixture_quantile(w, mu, stds, 1 - p)\n",
    "    return float(q), best, best[\"aic\"], best[\"bic\"]\n",
    "\n",
    "def empirical_quantile(sample, p):\n",
    "    # FM(p) = Q_{1-p} of residuals\n",
    "    return float(np.quantile(sample, 1 - p, method='linear' if int(np.__version__.split('.')[1]) >= 22 else 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfe49efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              count  mean    std     min    25%    50%    75%     max\n",
      "model                                                                \n",
      "MLR_linear  60483.0  -0.0  8.205 -71.254 -5.157 -0.565  4.157  48.320\n",
      "Poly2_OLS   60483.0  -0.0  7.164 -86.033 -4.247 -0.564  3.392  49.442\n"
     ]
    }
   ],
   "source": [
    "# ============================== Collect CV residuals ================================\n",
    "# Choose the model(s) you want to calibrate FM for\n",
    "SELECTED_MODELS_FOR_FM = [\"Poly2_OLS\", \"MLR_linear\"]  # keep OLS baseline + your main model\n",
    "\n",
    "def collect_cv_residuals(model_name, pipe, fold_assignments, sort_by_time=True):\n",
    "    rows = []\n",
    "    for fold in range(5):\n",
    "        tr_idx = np.where(fold_assignments != fold)[0]\n",
    "        val_idx = np.where(fold_assignments == fold)[0]\n",
    "\n",
    "        X_tr, y_tr_adj = X_lin_train[tr_idx], y_train_adj[tr_idx]\n",
    "        X_val, y_val, off_val, t_val = (\n",
    "            X_lin_train[val_idx],\n",
    "            y_train[val_idx],\n",
    "            offset_train[val_idx],\n",
    "            time_train[val_idx] if 'time_train' in locals() else np.arange(len(val_idx))\n",
    "        )\n",
    "\n",
    "        est = clone(pipe).fit(X_tr, y_tr_adj)\n",
    "        y_val_pred = est.predict(X_val) + off_val       # back to PL units\n",
    "        eps = y_val - y_val_pred                        # residuals (true - pred)\n",
    "\n",
    "        for e, tt in zip(eps, t_val):\n",
    "            rows.append((fold, tt, float(e)))\n",
    "    df = pd.DataFrame(rows, columns=[\"fold\", \"time\", \"epsilon\"])\n",
    "    if sort_by_time:\n",
    "        df = df.sort_values([\"fold\", \"time\"]).reset_index(drop=True)\n",
    "    df[\"model\"] = model_name\n",
    "    return df\n",
    "\n",
    "residuals_df_list = []\n",
    "for name in SELECTED_MODELS_FOR_FM:\n",
    "    residuals_df_list.append(collect_cv_residuals(name, models[name], fold_assignments))\n",
    "cv_residuals = pd.concat(residuals_df_list, ignore_index=True)\n",
    "print(cv_residuals.groupby(\"model\")[\"epsilon\"].describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bd1a9c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m     eps \u001b[38;5;241m=\u001b[39m cv_residuals\u001b[38;5;241m.\u001b[39mloc[cv_residuals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m model_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m P_GRID:\n\u001b[1;32m---> 61\u001b[0m         fm_rows\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mfm_with_uncertainty\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_parametric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m})\n\u001b[0;32m     62\u001b[0m fm_table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(fm_rows)\n\u001b[0;32m     63\u001b[0m display_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_emp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_emp_lo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_emp_hi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_param\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_param_lo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_param_hi\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselected\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_sel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_sel_lo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_sel_hi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock_len\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[31], line 39\u001b[0m, in \u001b[0;36mfm_with_uncertainty\u001b[1;34m(eps, p, use_parametric, B, random_state)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     stat_par \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: gmm_fit_quantile(np\u001b[38;5;241m.\u001b[39masarray(x), p)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 39\u001b[0m lo_p, hi_p, info_p \u001b[38;5;241m=\u001b[39m \u001b[43mbca_ci\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_par\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m out\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_param\u001b[39m\u001b[38;5;124m\"\u001b[39m: fm_par, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_param_lo\u001b[39m\u001b[38;5;124m\"\u001b[39m: lo_p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_param_hi\u001b[39m\u001b[38;5;124m\"\u001b[39m: hi_p,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: sel_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_bic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(chosen[\u001b[38;5;241m3\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_aic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(chosen[\u001b[38;5;241m2\u001b[39m]),\n\u001b[0;32m     43\u001b[0m })\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# By default, keep empirical as primary; switch to parametric if it is *meaningfully* larger at far tail\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# (i.e., conservatism for p<=0.02). You can change this policy.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 89\u001b[0m, in \u001b[0;36mbca_ci\u001b[1;34m(x, stat_fn, alpha, B, block_len, rng)\u001b[0m\n\u001b[0;32m     87\u001b[0m jack \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(n)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m---> 89\u001b[0m     jack[i] \u001b[38;5;241m=\u001b[39m \u001b[43mstat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m jack_mean \u001b[38;5;241m=\u001b[39m jack\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     91\u001b[0m num \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((jack_mean \u001b[38;5;241m-\u001b[39m jack)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 37\u001b[0m, in \u001b[0;36mfm_with_uncertainty.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     35\u001b[0m     stat_par \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: t_fit_quantile(np\u001b[38;5;241m.\u001b[39masarray(x), p)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m     stat_par \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mgmm_fit_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m lo_p, hi_p, info_p \u001b[38;5;241m=\u001b[39m bca_ci(eps, stat_par, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, B\u001b[38;5;241m=\u001b[39mB, block_len\u001b[38;5;241m=\u001b[39mblock_len)\n\u001b[0;32m     40\u001b[0m out\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_param\u001b[39m\u001b[38;5;124m\"\u001b[39m: fm_par, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_param_lo\u001b[39m\u001b[38;5;124m\"\u001b[39m: lo_p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfm_param_hi\u001b[39m\u001b[38;5;124m\"\u001b[39m: hi_p,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: sel_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_bic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(chosen[\u001b[38;5;241m3\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparam_aic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(chosen[\u001b[38;5;241m2\u001b[39m]),\n\u001b[0;32m     43\u001b[0m })\n",
      "Cell \u001b[1;32mIn[29], line 137\u001b[0m, in \u001b[0;36mgmm_fit_quantile\u001b[1;34m(sample, p, max_k)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    136\u001b[0m     gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39mk, covariance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE)\n\u001b[1;32m--> 137\u001b[0m     \u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     bic \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mbic(xs); aic \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39maic(xs)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (bic \u001b[38;5;241m<\u001b[39m best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbic\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\nahsh\\anaconda3\\envs\\general_env\\lib\\site-packages\\sklearn\\mixture\\_base.py:180\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nahsh\\anaconda3\\envs\\general_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nahsh\\anaconda3\\envs\\general_env\\lib\\site-packages\\sklearn\\mixture\\_base.py:246\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    244\u001b[0m     prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[1;32m--> 246\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_step(X, log_resp)\n\u001b[0;32m    248\u001b[0m     lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n",
      "File \u001b[1;32mc:\\Users\\nahsh\\anaconda3\\envs\\general_env\\lib\\site-packages\\sklearn\\mixture\\_base.py:305\u001b[0m, in \u001b[0;36mBaseMixture._e_step\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_e_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    290\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"E step.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(log_prob_norm), log_resp\n",
      "File \u001b[1;32mc:\\Users\\nahsh\\anaconda3\\envs\\general_env\\lib\\site-packages\\sklearn\\mixture\\_base.py:526\u001b[0m, in \u001b[0;36mBaseMixture._estimate_log_prob_resp\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate log probabilities and responsibilities for each sample.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03mCompute the log probabilities, weighted log probabilities per\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;124;03m    logarithm of the responsibilities\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    525\u001b[0m weighted_log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_weighted_log_prob(X)\n\u001b[1;32m--> 526\u001b[0m log_prob_norm \u001b[38;5;241m=\u001b[39m \u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweighted_log_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;66;03m# ignore underflow\u001b[39;00m\n\u001b[0;32m    529\u001b[0m     log_resp \u001b[38;5;241m=\u001b[39m weighted_log_prob \u001b[38;5;241m-\u001b[39m log_prob_norm[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "File \u001b[1;32mc:\\Users\\nahsh\\anaconda3\\envs\\general_env\\lib\\site-packages\\scipy\\special\\_logsumexp.py:118\u001b[0m, in \u001b[0;36mlogsumexp\u001b[1;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp_size(a) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# log of zero is OK\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m         out, sgn \u001b[38;5;241m=\u001b[39m \u001b[43m_logsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_sign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     shape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# NumPy is convenient for shape manipulation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nahsh\\anaconda3\\envs\\general_env\\lib\\site-packages\\scipy\\special\\_logsumexp.py:217\u001b[0m, in \u001b[0;36m_logsumexp\u001b[1;34m(a, b, axis, return_sign, xp)\u001b[0m\n\u001b[0;32m    214\u001b[0m shift \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mwhere(xp\u001b[38;5;241m.\u001b[39misfinite(a_max), a_max, xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39ma_max\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# Shift, exponentiate, scale, and sum\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m exp \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m*\u001b[39m xp\u001b[38;5;241m.\u001b[39mexp(a \u001b[38;5;241m-\u001b[39m shift) \u001b[38;5;28;01mif\u001b[39;00m b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m s \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39msum(exp, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mexp\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    219\u001b[0m s \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mwhere(s \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, s, s\u001b[38;5;241m/\u001b[39mm)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================== FM estimates + BCa CIs =============================\n",
    "def fm_with_uncertainty(eps, p, use_parametric=True, B=10_000, random_state=RANDOM_STATE):\n",
    "    eps = np.asarray(eps, float)\n",
    "    # Dependence diagnostics and block length\n",
    "    b, acf_info = choose_block_len_acf(eps)  # sort ~ time order if CV blocks were time-blocked\n",
    "    block_len = b if b >= 2 else 0\n",
    "\n",
    "    # Empirical quantile + BCa CI\n",
    "    stat_emp = lambda x: empirical_quantile(np.asarray(x), p)\n",
    "    lo_e, hi_e, info_e = bca_ci(eps, stat_emp, alpha=0.05, B=B, block_len=block_len)\n",
    "    fm_emp = info_e[\"theta_hat\"]\n",
    "\n",
    "    out = {\n",
    "        \"p\": p, \"block_len\": block_len, \"acf_info\": acf_info,\n",
    "        \"fm_emp\": fm_emp, \"fm_emp_lo\": lo_e, \"fm_emp_hi\": hi_e,\n",
    "        \"selected\": \"empirical\", \"fm_sel\": fm_emp, \"fm_sel_lo\": lo_e, \"fm_sel_hi\": hi_e,\n",
    "        \"estimator_note\": \"empirical quantile\"\n",
    "    }\n",
    "\n",
    "    # Optional parametric tail\n",
    "    if use_parametric:\n",
    "        # fit t and GMM; choose by BIC\n",
    "        fm_t, t_params, aic_t, bic_t = t_fit_quantile(eps, p)\n",
    "        fm_g, gmm_best, aic_g, bic_g = gmm_fit_quantile(eps, p)  # GMM is always available (sklearn)\n",
    "        # Decide among t and GMM\n",
    "        cand = []\n",
    "        if not (np.isnan(fm_t) or np.isinf(bic_t)): cand.append((\"t\", fm_t, aic_t, bic_t, t_params))\n",
    "        if gmm_best is not None: cand.append((\"gmm\", fm_g, aic_g, bic_g, gmm_best))\n",
    "        if cand:\n",
    "            chosen = min(cand, key=lambda z: z[3])  # min BIC\n",
    "            sel_name, fm_par, aic_sel, bic_sel, obj = chosen\n",
    "\n",
    "            # Parametric BCa CI: re-fit inside bootstrap\n",
    "            if sel_name == \"t\" and (sps is not None):\n",
    "                stat_par = lambda x: t_fit_quantile(np.asarray(x), p)[0]\n",
    "            else:\n",
    "                stat_par = lambda x: gmm_fit_quantile(np.asarray(x), p)[0]\n",
    "\n",
    "            lo_p, hi_p, info_p = bca_ci(eps, stat_par, alpha=0.05, B=B, block_len=block_len)\n",
    "            out.update({\n",
    "                \"fm_param\": fm_par, \"fm_param_lo\": lo_p, \"fm_param_hi\": hi_p,\n",
    "                \"param_name\": sel_name, \"param_bic\": float(chosen[3]), \"param_aic\": float(chosen[2]),\n",
    "            })\n",
    "\n",
    "            # By default, keep empirical as primary; switch to parametric if it is *meaningfully* larger at far tail\n",
    "            # (i.e., conservatism for p<=0.02). You can change this policy.\n",
    "            if p <= 0.02 and (fm_par > fm_emp):\n",
    "                out.update({\n",
    "                    \"selected\": f\"{sel_name}-tail\",\n",
    "                    \"fm_sel\": fm_par, \"fm_sel_lo\": lo_p, \"fm_sel_hi\": hi_p,\n",
    "                    \"estimator_note\": f\"{sel_name} tail (BIC-min)\"\n",
    "                })\n",
    "\n",
    "    return out\n",
    "\n",
    "# Calibrate per model, per p\n",
    "fm_rows = []\n",
    "for model_name in SELECTED_MODELS_FOR_FM:\n",
    "    eps = cv_residuals.loc[cv_residuals[\"model\"] == model_name, \"epsilon\"].values\n",
    "    for p in P_GRID:\n",
    "        fm_rows.append({\"model\": model_name, **fm_with_uncertainty(eps, p, use_parametric=True, B=800)})\n",
    "fm_table = pd.DataFrame(fm_rows)\n",
    "display_cols = [\"model\", \"p\", \"fm_emp\", \"fm_emp_lo\", \"fm_emp_hi\", \"param_name\", \"fm_param\", \"fm_param_lo\", \"fm_param_hi\",\n",
    "                \"selected\", \"fm_sel\", \"fm_sel_lo\", \"fm_sel_hi\", \"block_len\"]\n",
    "print(fm_table[display_cols].round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Held-out validation (Test) =========================\n",
    "def predict_pl(est, X, offset):\n",
    "    return est.predict(X) + offset\n",
    "\n",
    "def achieved_outage_rate(eps_test, FM):\n",
    "    return float(np.mean(eps_test > FM))\n",
    "\n",
    "def pdr_curve(eps_test, fm_min=0.0, fm_max=None, num=60):\n",
    "    if fm_max is None:\n",
    "        fm_max = max(HEURISTIC_FM_DB, np.percentile(eps_test, 99.9))\n",
    "    grid = np.linspace(fm_min, fm_max, num=num)\n",
    "    # achieved PDR = 1 - outage = empirical CDF of residuals at FM\n",
    "    pdr = np.array([np.mean(eps_test <= m) for m in grid], float)\n",
    "    return grid, pdr\n",
    "\n",
    "validation_rows = []\n",
    "for model_name in SELECTED_MODELS_FOR_FM:\n",
    "    est = fitted_models[model_name]\n",
    "    y_pred_test = predict_pl(est, X_lin_test, offset_test)\n",
    "    eps_test = y_test - y_pred_test\n",
    "\n",
    "    # FM at target ps\n",
    "    sub = fm_table[fm_table[\"model\"] == model_name].copy()\n",
    "    for _, r in sub.iterrows():\n",
    "        FM = r[\"fm_sel\"]\n",
    "        p = r[\"p\"]\n",
    "        phat = achieved_outage_rate(eps_test, FM)\n",
    "        validation_rows.append({\n",
    "            \"model\": model_name, \"p_target\": p,\n",
    "            \"FM_used\": FM, \"FM_lo\": r[\"fm_sel_lo\"], \"FM_hi\": r[\"fm_sel_hi\"],\n",
    "            \"achieved_outage\": phat, \"achieved_PDR\": 1.0 - phat,\n",
    "            \"estimator\": r[\"selected\"]\n",
    "        })\n",
    "\n",
    "    # Heuristic FM baseline\n",
    "    phat_h = achieved_outage_rate(eps_test, HEURISTIC_FM_DB)\n",
    "    validation_rows.append({\n",
    "        \"model\": model_name, \"p_target\": None,\n",
    "        \"FM_used\": HEURISTIC_FM_DB, \"FM_lo\": np.nan, \"FM_hi\": np.nan,\n",
    "        \"achieved_outage\": phat_h, \"achieved_PDR\": 1.0 - phat_h,\n",
    "        \"estimator\": f\"Heuristic ({HEURISTIC_FM_DB} dB)\"\n",
    "    })\n",
    "\n",
    "validation_df = pd.DataFrame(validation_rows)\n",
    "print(validation_df.sort_values([\"model\", \"p_target\"], na_position='last').round(4).to_string(index=False))\n",
    "\n",
    "# Build a dense PDR vs FM curve for the *main* model (first in the list)\n",
    "MAIN_MODEL_FOR_FIG = SELECTED_MODELS_FOR_FM[0]\n",
    "est = fitted_models[MAIN_MODEL_FOR_FIG]\n",
    "eps_test_main = y_test - predict_pl(est, X_lin_test, offset_test)\n",
    "fm_grid, pdr_grid = pdr_curve(eps_test_main, num=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Plot: PDR vs FM ===================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4.2))\n",
    "plt.plot(fm_grid, pdr_grid, label=f\"PDR curve – {MAIN_MODEL_FOR_FIG}\")\n",
    "# vertical lines at CV-calibrated FM(p) and horizontal lines at 1-p\n",
    "for p in P_GRID:\n",
    "    row = fm_table[(fm_table[\"model\"] == MAIN_MODEL_FOR_FIG) & (fm_table[\"p\"] == p)].iloc[0]\n",
    "    fm_p = row[\"fm_sel\"]\n",
    "    plt.axvline(fm_p, linestyle='--', linewidth=1.0)\n",
    "    plt.axhline(1.0 - p, linestyle=':', linewidth=1.0)\n",
    "\n",
    "plt.title(\"Achieved PDR vs Fade Margin (FM)\")\n",
    "plt.xlabel(\"Fade Margin FM (dB)\")\n",
    "plt.ylabel(\"Achieved PDR on Held-out Test\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, which='both', linewidth=0.5, alpha=0.3)\n",
    "plt.legend()\n",
    "os.makedirs(\"Reports/FM\", exist_ok=True)\n",
    "fig_path = \"Reports/FM/pdr_vs_fm.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved figure:\", fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Save CSV summaries ================================\n",
    "os.makedirs(\"Reports/FM\", exist_ok=True)\n",
    "\n",
    "fm_csv = \"Reports/FM/fm_calibration_summary.csv\"\n",
    "validation_csv = \"Reports/FM/fm_validation_summary.csv\"\n",
    "fm_table.to_csv(fm_csv, index=False)\n",
    "validation_df.to_csv(validation_csv, index=False)\n",
    "\n",
    "print(\"Saved:\", fm_csv)\n",
    "print(\"Saved:\", validation_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================  snippets ==============================\n",
    "def paper_snippet(model_name, p=0.01):\n",
    "    sub = fm_table[(fm_table[\"model\"] == model_name) & (fm_table[\"p\"] == p)].iloc[0]\n",
    "    val = validation_df[(validation_df[\"model\"] == model_name) & (validation_df[\"p_target\"] == p)].iloc[0]\n",
    "    fm = sub[\"fm_sel\"]; lo = sub[\"fm_sel_lo\"]; hi = sub[\"fm_sel_hi\"]; est = sub[\"selected\"]\n",
    "    ach_p = val[\"achieved_outage\"]; ach_pdr = val[\"achieved_PDR\"]\n",
    "    return (\n",
    "        f\"FM calibration for {model_name}: For p={p:.2%}, we obtain FM99 = {fm:.2f} dB \"\n",
    "        f\"[95% BCa: {lo:.2f}, {hi:.2f}] using {est}. On held-out test, achieved outage \"\n",
    "        f\"is {ach_p:.2%} (PDR={ach_pdr:.2%}), lying on the target iso-line.\"\n",
    "    )\n",
    "\n",
    "for m in SELECTED_MODELS_FOR_FM:\n",
    "    print(paper_snippet(m, p=0.01))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
